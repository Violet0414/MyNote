#### 初步认识虚拟背景

虚拟背景往往在视频通话过程中被使用，其过程涉及了人物动态计算、人像抠图、背景填充（增加马赛克或者其他的色彩）等等。

核心步骤：

- 识别当前画面中的人；

- 动态从这个画面中扣出第一步识别出的人的画面；

- 给非人部分增加马赛克或者其他的背景。

此处将使用`MediaPipe`实现虚拟背景的功能。



#### MediaPipe简介

​	[MediaPipe ](https://link.juejin.cn/?target=https%3A%2F%2Fgoogle.github.io%2Fmediapipe%2F)是谷歌开源的适用于多平台、终端的机器学习框架，其内部有很多的工具包和基础解决方案，安装即可使用，内部使用的模型也有开源的。像人脸检测、面部识别、虹膜、手势、姿态、人体、人体分割、头发分割、3D识别等常见场景，都可以直接找到对应的成熟解决案例和模型。

因此利用上述框架中的人体分割模型，就可以实现我们在摄像头中的画面人物和背景分割的目标。分割完成后，还可以利用其他强大的功能，对已经分割识别的动态流自定义处理，进而实现背景自定义。



#### 实现流程

**1.安装JS版本的MediaPipe中人体分割相关的依赖库。**

```js
npm i @mediapipe/selfie_segmentation
```



**2.视频流初始化**

```js
/**
 * 获取指定媒体设备id对应的媒体流（不传参数则获取默认的摄像头和麦克风）
 * @author suke
 * @param videoId
 * @param audioId
 * @returns {Promise<void>}
 */
async getTargetDeviceMedia(videoId,audioId){
    const constraints = {
        audio: {deviceId: audioId ? {exact: audioId} : undefined},
        video: {
            deviceId: videoId ? {exact: videoId} : undefined,
            width:1920,
            height:1080,
            frameRate: { ideal: 10, max: 15 }
        }
    };
    if (window.stream) {
        window.stream.getTracks().forEach(track => {
            track.stop();
        });
    }
    //被调用方法前面有，此处不再重复
    return await this.getLocalUserMedia(constraints).catch(handleError);
},
```



**3.初始化图像分割工具**

​	以下代码中出现了一个 `canvas` 元素，这个载体我们作为拿到虚拟背景后将对应画面展示的地方。

​	同时可以看到，有个地方用到了动态地址，这个动态地址就是下载具体版本模型的地方，因为 cdn地址在国内访问比较慢，因此我将其下载到本地，然后通过 `nginx` 代理通过区域网访问对应模型。

```js

initVb(){
    canvasElement = document.getElementById('output_canvas');
    canvasCtx = canvasElement.getContext('2d');
    image = new Image();
    image.src = this.meimage
    selfieSegmentation = new SFS.SelfieSegmentation({locateFile: (file) => {
      console.log(file);
      return `http://192.168.101.138:8080/${file}`;//ng  代理模型文件夹
      // return `https://cdn.jsdelivr.'net/npm/@mediapipe/selfie_segmentation@0.1.1632777926/${file}`;
    }});                                
    selfieSegmentation.setOptions({
      modelSelection: 1,
      minDetectionConfidence: 0.5,
      minTrackingConfidence: 0.5,
    });
    selfieSegmentation.onResults(this.handleResults);
},
```



**4.图像分割后处理背景和人像**

​	在前面的官方 `Demo` 中，并没有设置背景的，仅仅是将分割后的人像使用特定的颜色框出来，这里大家可以和官方的案例中对比下。

```js
handleResults(results) {
    // Prepare the new frame
    canvasCtx.save();
    canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);
    canvasCtx.drawImage(results.segmentationMask, 0, 0, canvasElement.width, canvasElement.height);
   //利用canvas绘制新背景 
   //canvasCtx.globalCompositeOperation = 'source-in';则意味着处理分割后图像中的人体。 
    canvasCtx.globalCompositeOperation = 'source-out';
    canvasCtx.drawImage(image, 0, 0, image.width, image.height, 0, 0, canvasElement.width, canvasElement.height);
    canvasCtx.globalCompositeOperation = 'destination-atop';
    canvasCtx.drawImage(results.image, 0, 0, canvasElement.width, canvasElement.height);
    // Done
    canvasCtx.restore();
},
```



**5.监听流播放后触发上述工具模型处理画面，并绘制到前面声明的 Canvas 载体。**

```js
// 监听触发模型处理
async virtualBg(){
  const that = this
  let video = document.getElementById('localdemo01')
  video.addEventListener('playing',function(){
    let myvideo = this;
    let lastTime = new Date();
    async function getFrames() {
      const now = myvideo.currentTime;
      if(now > lastTime){
        await selfieSegmentation.send({image: myvideo});
      }
      lastTime = now;
      //无限定时循环 退出记得取消 cancelAnimationFrame() 
      requestAnimationFrame(getFrames);
    };
    getFrames()
  })
}
```



#### 总结

1. 获取摄像头画面流。

2. 初始化图像分割工具。

3. 在本地的页面 DOM 中，播放第一步获取到的视频流。

4. 监听视频流播放后，将画面帧发送到图像分割工具处理。

5. 图像分割工具利用机器学习模型，识别画面并分割人体，然后处理得到分割后的蒙版，我们得到蒙版后将背景替换成自己的图片，最后展示到 canvas 。

初始化图像分割工具时有几个参数配置，这里挑几个重要的说明下。

- `MIN_DETECTION_CONFIDENCE` ：手部检测模型中的最小置信度值，取值区间`[0.0, 1.0]` 被认为是成功的检测。默认为`0.5`。

- `MIN_TRACKING_CONFIDENCE` ： 跟踪模型的最小置信度值，取值区间`[0.0, 1.0]`，将其设置为更高的值可以提高解决方案的稳健性，但是会带来更高的延迟，默认`0.5`。









