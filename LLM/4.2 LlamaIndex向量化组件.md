### Embeddings(å‘é‡åŒ–ç»„ä»¶)

**æ ¸å¿ƒåŸºç±»**

```python
from llama_index.core.embeddings import BaseEmbedding

class BaseEmbedding:
    """æ‰€æœ‰embeddingæ¨¡å‹çš„åŸºç±»"""
    
    def get_text_embedding(self, text: str) -> List[float]:
        """å•ä¸ªæ–‡æœ¬å‘é‡åŒ–"""
        
    def get_text_embedding_batch(self, texts: List[str]) -> List[List[float]]:
        """æ‰¹é‡æ–‡æœ¬å‘é‡åŒ–"""
        
    def get_agg_embedding_from_queries(self, *args) -> List[float]:
        """ä»æŸ¥è¯¢èšåˆembedding"""
```



**äº‘ç«¯APIæ¨¡å‹**

```python
# OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding
embed_model = OpenAIEmbedding(
    model="text-embedding-3-small",
    api_key="sk-...",
    embed_batch_size=100  # æ‰¹é‡å¤§å°
)

# Azure OpenAI
from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding
embed_model = AzureOpenAIEmbedding(
    model="text-embedding-3-small",
    deployment_name="your-deployment",
    api_key="...",
    azure_endpoint="https://xxx.openai.azure.com/"
)

# Cohere
from llama_index.embeddings.cohere import CohereEmbedding
embed_model = CohereEmbedding(
    cohere_api_key="...",
    model_name="embed-english-v3.0",
    input_type="search_document"
)

# ç™¾åº¦æ–‡å¿ƒ
from llama_index.embeddings.baidu_qianfan import QianfanEmbedding
embed_model = QianfanEmbedding(
    model="Embedding-V1",
    qianfan_ak="...",
    qianfan_sk="..."
)
```



**æœ¬åœ°æ¨¡å‹**

```python
# HuggingFace
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
embed_model = HuggingFaceEmbedding(
    model_name="BAAI/bge-large-zh-v1.5",
    device="cuda",  # å¯é€‰ï¼šcuda, cpu, mps
    trust_remote_code=True,
    cache_folder="./hf_cache"
)

# Sentence Transformers
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
embed_model = HuggingFaceEmbedding(
    model_name="sentence-transformers/all-mpnet-base-v2",
    normalize_embeddings=True
)

# è‡ªå®šä¹‰æœ¬åœ°æ¨¡å‹
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
embed_model = HuggingFaceEmbedding(
    model_name="/path/to/your/model",
    tokenizer_name="/path/to/your/tokenizer"
)
```

---



**å‘é‡åŒ–**

```python
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
import torch

# ä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„
embed_model = HuggingFaceEmbedding(
    model_name="./model",  # æœ¬åœ°æ¨¡å‹æ–‡ä»¶å¤¹è·¯å¾„
    device="cuda" if torch.cuda.is_available() else "cpu",  # è‡ªåŠ¨é€‰æ‹©è®¾å¤‡
    trust_remote_code=True,  # å¦‚æœæ¨¡å‹éœ€è¦è‡ªå®šä¹‰ä»£ç 
    model_kwargs={
        "torch_dtype": torch.float16,  # åŠç²¾åº¦èŠ‚çœå†…å­˜
        "local_files_only": True,      # å¼ºåˆ¶ä½¿ç”¨æœ¬åœ°æ–‡ä»¶
    },
    encode_kwargs={
        "normalize_embeddings": True,  # å½’ä¸€åŒ–å‘é‡
        "batch_size": 32,
        "show_progress_bar": True
    }
)

# æµ‹è¯•å‘é‡åŒ–
texts = ["è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å¥å­", "è¿™æ˜¯å¦ä¸€ä¸ªå¥å­"]
embeddings = embed_model.get_text_embedding_batch(texts)

print(f"Embeddingç»´åº¦: {len(embeddings[0])}")
print(f"å‰5ä¸ªå€¼: {embeddings[0][:5]}")
```



## åŸºæœ¬äº¤äº’ç¤ºä¾‹

```python
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.llms.openai import OpenAI
from llama_index.core import Settings
import os

# 1. é…ç½® LLMï¼ˆè¿™é‡Œä»¥ OpenAI ä¸ºä¾‹ï¼‰
# è®¾ç½®ä½ çš„ OpenAI API key
os.environ["OPENAI_API_KEY"] = "your-api-key"

# åˆ›å»º LLM å®ä¾‹
llm = OpenAI(model="gpt-3.5-turbo", temperature=0.7)

# è®¾ç½®ä¸ºå…¨å±€ LLM
Settings.llm = llm

# 2. åŠ è½½æ–‡æ¡£ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸éœ€è¦ RAG å¯ä»¥è·³è¿‡ï¼‰
# ä»æ–‡ä»¶å¤¹åŠ è½½æ–‡æ¡£
documents = SimpleDirectoryReader("./data").load_data()

# 3. åˆ›å»ºç´¢å¼•
index = VectorStoreIndex.from_documents(documents)

# 4. åˆ›å»ºæŸ¥è¯¢å¼•æ“
query_engine = index.as_query_engine()

# 5. äº¤äº’å¼é—®ç­”
while True:
    # è·å–ç”¨æˆ·è¾“å…¥
    query = input("\nè¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼ˆè¾“å…¥ 'quit' é€€å‡ºï¼‰: ")
    
    if query.lower() in ['quit', 'exit', 'q']:
        print("å†è§ï¼")
        break
    
    if not query.strip():
        continue
    
    # æ‰§è¡ŒæŸ¥è¯¢
    try:
        print("\nğŸ¤– æ­£åœ¨æ€è€ƒ...")
        response = query_engine.query(query)
        print(f"\nğŸ’¡ å›ç­”: {response}")
    except Exception as e:
        print(f"âŒ å‡ºé”™: {e}")
```



## å¦‚æœä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼ˆå¦‚ HuggingFaceï¼‰

```python
from llama_index.core import Settings
from llama_index.llms.huggingface import HuggingFaceLLM
from llama_index.core import PromptTemplate
import torch

# 1. é…ç½®æœ¬åœ° LLM
llm = HuggingFaceLLM(
    model_name="Qwen/Qwen2-7B-Instruct",  # æˆ–ä½¿ç”¨æœ¬åœ°è·¯å¾„ "./model"
    tokenizer_name="Qwen/Qwen2-7B-Instruct",
    context_window=4096,
    max_new_tokens=256,
    generate_kwargs={"temperature": 0.7, "do_sample": True},
    device_map="auto",
)

# è®¾ç½®ä¸ºå…¨å±€ LLM
Settings.llm = llm

# 2. ç®€å•çš„ç›´æ¥é—®ç­”ï¼ˆä¸ä½¿ç”¨ RAGï¼‰
def simple_chat():
    """ç®€å•çš„å¯¹è¯äº¤äº’"""
    print("ğŸ¤– æœ¬åœ°æ¨¡å‹èŠå¤©æœºå™¨äººï¼ˆè¾“å…¥ 'quit' é€€å‡ºï¼‰")
    print("=" * 50)
    
    while True:
        try:
            # è·å–ç”¨æˆ·è¾“å…¥
            user_input = input("\nğŸ‘¤ ä½ : ").strip()
            
            if user_input.lower() in ['quit', 'exit', 'q']:
                print("ğŸ¤– å†è§ï¼")
                break
            
            if not user_input:
                continue
            
            # æ„é€ æç¤ºè¯
            prompt = f"""è¯·å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š
            
é—®é¢˜ï¼š{user_input}

å›ç­”ï¼š"""
            
            # ç”Ÿæˆå›ç­”
            print("ğŸ¤– æ­£åœ¨æ€è€ƒ...")
            response = llm.complete(prompt)
            
            print(f"ğŸ¤– æœºå™¨äºº: {response.text}")
            
        except KeyboardInterrupt:
            print("\nğŸ¤– å†è§ï¼")
            break
        except Exception as e:
            print(f"âŒ å‡ºé”™: {e}")

# è¿è¡ŒèŠå¤©
if __name__ == "__main__":
    simple_chat()
```



## ä½¿ç”¨æœ¬åœ°å‘é‡çŸ¥è¯†çš„ RAG é—®ç­”

```python
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.llms.huggingface import HuggingFaceLLM
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core import Settings
import torch

# 1. é…ç½®æœ¬åœ°æ¨¡å‹
print("ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹...")

# é…ç½® LLM
llm = HuggingFaceLLM(
    model_name="Qwen/Qwen2-7B-Instruct",  # å¯æ›¿æ¢ä¸ºä½ çš„æœ¬åœ°æ¨¡å‹è·¯å¾„
    tokenizer_name="Qwen/Qwen2-7B-Instruct",
    context_window=4096,
    max_new_tokens=512,
    device_map="auto",
)

# é…ç½® Embedding æ¨¡å‹
embed_model = HuggingFaceEmbedding(
    model_name="BAAI/bge-small-zh-v1.5",  # å¯æ›¿æ¢ä¸ºä½ çš„æœ¬åœ° embedding æ¨¡å‹
    device="cuda" if torch.cuda.is_available() else "cpu",
)

# è®¾ç½®å…¨å±€é…ç½®
Settings.llm = llm
Settings.embed_model = embed_model
Settings.chunk_size = 512
Settings.chunk_overlap = 50

# 2. åŠ è½½æœ¬åœ°çŸ¥è¯†åº“
print("ğŸ“š æ­£åœ¨åŠ è½½çŸ¥è¯†åº“...")
try:
    # ä» data æ–‡ä»¶å¤¹åŠ è½½æ–‡æ¡£
    documents = SimpleDirectoryReader("./data").load_data()
    
    # åˆ›å»ºç´¢å¼•
    index = VectorStoreIndex.from_documents(
        documents,
        show_progress=True
    )
    
    # åˆ›å»ºæŸ¥è¯¢å¼•æ“
    query_engine = index.as_query_engine(
        similarity_top_k=3,  # ä½¿ç”¨å‰3ä¸ªæœ€ç›¸å…³çš„æ–‡æ¡£
        verbose=True  # æ˜¾ç¤ºè¯¦ç»†è¿‡ç¨‹
    )
    
    print("âœ… ç³»ç»Ÿå‡†å¤‡å°±ç»ªï¼")
    
except Exception as e:
    print(f"âŒ åŠ è½½çŸ¥è¯†åº“å¤±è´¥: {e}")
    print("âš ï¸  å°†ä½¿ç”¨æ— çŸ¥è¯†åº“æ¨¡å¼")
    query_engine = None

# 3. äº¤äº’å¼é—®ç­”
def rag_chat():
    """åŸºäºçŸ¥è¯†åº“çš„é—®ç­”"""
    print("\n" + "="*50)
    print("ğŸ¤– æ™ºèƒ½çŸ¥è¯†é—®ç­”ç³»ç»Ÿ")
    print("="*50)
    print("æç¤ºï¼š")
    print("1. è¾“å…¥é—®é¢˜è·å–åŸºäºçŸ¥è¯†åº“çš„å›ç­”")
    print("2. è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
    print("="*50)
    
    while True:
        try:
            # è·å–ç”¨æˆ·é—®é¢˜
            question = input("\nâ“ è¯·è¾“å…¥é—®é¢˜: ").strip()
            
            if question.lower() in ['quit', 'exit', 'q']:
                print("ğŸ¤– æ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§ï¼")
                break
            
            if not question:
                continue
            
            print("\nğŸ” æ­£åœ¨æ£€ç´¢ä¿¡æ¯...")
            
            if query_engine:
                # ä½¿ç”¨çŸ¥è¯†åº“å›ç­”é—®é¢˜
                response = query_engine.query(question)
                print(f"\nğŸ’¡ å›ç­”: {response}")
                
                # æ˜¾ç¤ºå‚è€ƒæ¥æºï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if hasattr(response, 'source_nodes') and response.source_nodes:
                    print("\nğŸ“š å‚è€ƒæ¥æº:")
                    for i, node in enumerate(response.source_nodes[:2]):  # æ˜¾ç¤ºå‰2ä¸ªæ¥æº
                        print(f"{i+1}. {node.text[:150]}...")
            else:
                # æ— çŸ¥è¯†åº“ï¼Œç›´æ¥ä½¿ç”¨ LLM
                prompt = f"""è¯·å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š

é—®é¢˜ï¼š{question}

å›ç­”ï¼š"""
                response = llm.complete(prompt)
                print(f"\nğŸ’¡ å›ç­”: {response.text}")
                
        except KeyboardInterrupt:
            print("\nğŸ¤– å†è§ï¼")
            break
        except Exception as e:
            print(f"âŒ å‡ºé”™: {e}")

# è¿è¡Œ
if __name__ == "__main__":
    rag_chat()
```



## æœ€ç®€å•ç›´æ¥çš„ç‰ˆæœ¬ï¼ˆæ— çŸ¥è¯†åº“ï¼‰

```python
from llama_index.llms.huggingface import HuggingFaceLLM
import torch

# 1. åˆå§‹åŒ–æ¨¡å‹
print("ğŸš€ æ­£åœ¨åŠ è½½æ¨¡å‹...")
llm = HuggingFaceLLM(
    model_name="Qwen/Qwen2-7B-Instruct",  # æ›¿æ¢ä¸ºä½ çš„æ¨¡å‹è·¯å¾„
    tokenizer_name="Qwen/Qwen2-7B-Instruct",
    device_map="auto",
)

print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")

# 2. ç®€å•çš„é—®ç­”å¾ªç¯
while True:
    question = input("\nğŸ‘¤ ä½ é—®: ").strip()
    
    if question.lower() in ['é€€å‡º', 'quit', 'exit', 'q']:
        print("ğŸ¤– å†è§ï¼")
        break
    
    if not question:
        continue
    
    try:
        print("ğŸ¤– æ­£åœ¨æ€è€ƒ...")
        
        # ç›´æ¥ä½¿ç”¨ LLM ç”Ÿæˆå›ç­”
        response = llm.complete(question)
        
        print(f"ğŸ¤– å›ç­”: {response.text}")
        
    except Exception as e:
        print(f"âŒ å‡ºé”™: {e}")
```

---