### 相关的一些名词概念

#### **学习率**

​		模型每次更新时权重改变的幅度，学习率较大可能会错过最优解，学习率较小会导致模型学习较慢或者陷入局部最优解，类似开车一脚油门踩大了会冲出终点，踩小了又可能过很久才能到终点。

​		**学习率调节**：设置模型的学习速率，**一般来说，模型学习率在一开始会设置的比较大，在快结束时会将学习率降低，防止过拟合的情况出现**。就好像在开车前往一个未知而不明确的目的地时，往往开始都会速度很快，而快到地方时则需要降下速度慢慢去寻找目的地点。



#### **训练轮数**

​		顾名思义，模型的训练次数，轮数过低可能会导致模型出现欠拟合的情况，而训练轮次过多则有可能导致模型过拟合。

​				**欠拟合**：未完全学习到足够的知识，没有将训练数据的核心特征记录下来。

​				**过拟合**：在当前训练数据上表现的很好，但超出当前数据的问题则有可能表现得很差，出现该情况的原因就是大模型过于了解当前的训练数据，导致其将训练数据中一些无关紧要的特征都记录了下来，失去了举一反三的能力。



#### **样本数**

​		训练集中的数据，训练过程中如果样本数过大，可以设置一个最大样本数，让模型通过随机采样的策略去进行训练。



#### **计算类型**

​		在模型进行训练时使用的数据类型，**常见的计算类型有float 32 和float 16，使用float 16的话可以减少内存的占用，训练速度更快，但训练精度会有所损失，通常来说计算类型就是在性能和精度之间去做出一个平衡。**



#### **截断长度**

​		输入序列分词后的最大长度，如过太小可能会丢失一些重要的数据信息，设置的太长有可能会出现内存溢出。



#### **批处理大小**

​		每个GPU处理的样本数量，较大的批次可以加速计算，但占用的内存肯定更多。



#### **训练集/验证集**

​		

#### 损失函数

​		模型代表的数据分布与真实的数据分布之间会存在一定的差异，我们以一个函数去代表这个误差，这个函数就是损失函数，有时也被称为误差函数。当我们找到了该函数函数值最小的位置，就等于找到了接近正确的分布。



#### **梯度**

​		由于在模型训练过程中，损失函数往往较为复杂，无法通过简单的计算找到函数值的最小位置，这时则可以通过梯度指出方向一步一步找到最小位置。

​		**梯度**在深度学习中是一个**多维向量**，它表示函数在某个点的**变化率最快的方向和大小**。对于LLM这样的神经网络：

- 梯度指向**损失函数增长最快的方向**
- 在训练中，我们通常沿着**梯度的负方向**更新参数，使损失减小



**梯度消失/爆炸**

- **问题**：深层网络中梯度可能指数级减小或增大
- **LLM解决方案**：
  - 残差连接（Residual Connections）
  - 梯度裁剪（Gradient Clipping）
  - 合理的初始化



**残差连接**		

```reStructuredText
传统：H(x) = 目标映射
残差：H(x) = F(x) + x
其中 F(x) = H(x) - x （需要学习的残差）
```

- **本质**：学习输入与输出的差值

- **作用**：解决了深度网络的梯度传播问题

- **结果**：使得训练极深层网络成为可能

- **应用**：几乎所有SOTA模型都在使用

  ​	在LLM中，残差连接（结合LayerNorm）使得Transformer可以堆叠数十甚至数百层，从而获得强大的表示能力。



**梯度裁剪**

​		梯度裁剪是深度学习中一种重要的**优化技术**，用于**防止梯度爆炸**，保证训练稳定性。在LLM训练中尤其关键，因为Transformer模型容易产生大的梯度。当梯度的范数超过某个阈值时，将其**按比例缩小**，使其不超过该阈值。

```reStructuredText
原始梯度可能很大：▽L = [100, -200, 150] （范数很大）
梯度裁剪后：▽L_clipped = [40, -80, 60] （缩放到阈值内）
```

**两种主要裁剪方法**

1. **按值裁剪（Value Clipping）**

```python
def clip_grad_value(parameters, clip_value):
    """将每个梯度元素裁剪到[-clip_value, clip_value]范围内"""
    for param in parameters:
        if param.grad is not None:
            param.grad.data.clamp_(-clip_value, clip_value)
```

**效果**：`grad = max(min(grad, clip_value), -clip_value)`

2. **按范数裁剪（Norm Clipping）**

```python
def clip_grad_norm(parameters, max_norm, norm_type=2):
    """将梯度向量的范数裁剪到max_norm"""
    total_norm = 0
    for p in parameters:
        if p.grad is not None:
            param_norm = p.grad.data.norm(norm_type)
            total_norm += param_norm ** norm_type
    
    total_norm = total_norm ** (1./norm_type)
    
    # 计算缩放因子
    clip_coef = max_norm / (total_norm + 1e-6)
    
    # 应用裁剪
    if clip_coef < 1:
        for p in parameters:
            if p.grad is not None:
                p.grad.data.mul_(clip_coef)
    
    return total_norm
```





















​		