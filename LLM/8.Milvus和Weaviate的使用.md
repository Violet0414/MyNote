## 一、Milvus

### 特点

- **专门为向量搜索设计**：专注于大规模向量相似性搜索
- **云原生架构**：支持分布式部署，可水平扩展
- **多种索引算法**：IVF_FLAT、HNSW、IVF_PQ、SCANN等
- **支持GPU加速**：利用GPU提高搜索性能

### 基本使用

#### 1. 安装

```bash
# Docker方式
docker pull milvusdb/milvus:latest
docker run -d --name milvus -p 19530:19530 -p 9091:9091 milvusdb/milvus:latest
```



#### 2. Python客户端使用

```python
from pymilvus import connections, Collection, FieldSchema, CollectionSchema, DataType

# 连接
connections.connect(host='localhost', port='19530')

# 定义schema
fields = [
    FieldSchema(name="id", dtype=DataType.INT64, is_primary=True),
    FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=128)
]
schema = CollectionSchema(fields, description="向量集合")

# 创建集合
collection = Collection(name="test_collection", schema=schema)

# 创建索引
index_params = {
    "metric_type": "L2",
    "index_type": "IVF_FLAT",
    "params": {"nlist": 128}
}
collection.create_index("embedding", index_params)

# 插入数据
import numpy as np
vectors = np.random.random((1000, 128)).tolist()
ids = list(range(1000))
collection.insert([ids, vectors])

# 搜索
search_params = {"metric_type": "L2", "params": {"nprobe": 10}}
results = collection.search(
    vectors[:5],  # 查询向量
    "embedding",
    search_params,
    limit=10
)
```



## 二、Weaviate

### 特点

- **多模态支持**：文本、图片、视频等
- **内置模块**：支持各种embedding模型、生成式AI功能
- **GraphQL接口**：灵活的查询语言
- **更像数据库**：支持CRUD、过滤、聚合等

### 基本使用

#### 1. 安装

```bash
# Docker方式
docker run -d \
  -p 8080:8080 \
  -p 50051:50051 \
  --name weaviate \
  semitechnologies/weaviate:latest
```



#### 2. Python客户端使用

```python
import weaviate
from weaviate.classes.config import Configure

# 连接客户端
client = weaviate.connect_to_local()

# 创建集合（Class）
client.collections.create(
    name="Article",
    vectorizer_config=Configure.Vectorizer.text2vec_transformers(),
    properties=[
        {
            "name": "title",
            "data_type": "text"
        },
        {
            "name": "content",
            "data_type": "text"
        }
    ]
)

# 获取集合
articles = client.collections.get("Article")

# 插入数据
articles.data.insert({
    "title": "AI的突破",
    "content": "最新的人工智能研究..."
})

# 向量搜索
response = articles.query.near_vector(
    near_vector=[0.1]*768,  # 查询向量
    limit=5
)

for obj in response.objects:
    print(obj.properties["title"])
    print(f"距离: {obj.metadata.distance}")

client.close()
```





#### 3. GraphQL查询示例

```graphql
{
  Get {
    Article(
      nearVector: {
        vector: [0.1, 0.2, ...]
      }
      where: {
        path: ["title"]
        operator: Like
        valueString: "*AI*"
      }
      limit: 10
    ) {
      title
      content
      _additional {
        distance
      }
    }
  }
}
```



## 三、两者对比

| 特性           | Milvus              | Weaviate           |
| :------------- | :------------------ | :----------------- |
| **核心定位**   | 高性能向量搜索引擎  | 多模态AI原生数据库 |
| **查询语言**   | Python SDK/REST API | GraphQL/REST API   |
| **向量化**     | 需要外部生成        | 内置多种向量化模块 |
| **数据类型**   | 主要处理向量        | 支持多种数据类型   |
| **生态系统**   | 与AI/ML工具链集成好 | 更像传统数据库     |
| **部署复杂度** | 较高（分布式）      | 相对简单           |
| **云服务**     | Zilliz Cloud        | Weaviate Cloud     |



## 四、选择建议

### 选择 Milvus 当：

- 需要处理**十亿级**向量数据
- 对**搜索性能**有极致要求
- 场景主要是**向量检索**
- 需要GPU加速

### 选择 Weaviate 当：

- 需要**多模态**数据支持
- 希望**内置向量化**功能
- 需要丰富的**过滤和聚合**功能
- 更喜欢**GraphQL**查询接口
- 需要**生成式AI**集成



## 五、实际应用示例

### 结合 OpenAI 的 RAG 系统

```python
# 使用 Weaviate 构建 RAG
import openai
import weaviate
from weaviate.classes.config import Configure

# 连接 Weaviate（使用 OpenAI 向量化）
client = weaviate.connect_to_weaviate_cloud(
    cluster_url="your-url",
    auth_credentials=weaviate.auth.AuthApiKey("your-key"),
    headers={
        "X-OpenAI-Api-Key": "your-openai-key"
    }
)

# 创建使用 OpenAI 向量化的集合
client.collections.create(
    name="Document",
    vectorizer_config=Configure.Vectorizer.text2vec_openai(
        model="text-embedding-ada-002"
    )
)

# 搜索并生成回答
docs = client.collections.get("Document")
results = docs.query.near_text(
    query="人工智能的未来发展",
    limit=3
)

# 使用检索结果生成回答
context = "\n".join([obj.properties["content"] for obj in results.objects])
response = openai.ChatCompletion.create(
    model="gpt-4",
    messages=[
        {"role": "system", "content": "基于以下文档回答问题："},
        {"role": "user", "content": f"文档：{context}\n\n问题：人工智能的未来发展？"}
    ]
)
```



## 六、性能优化建议

### Milvus：

1. **索引选择**：根据数据规模和精度要求选择
2. **参数调优**：调整 nlist、nprobe 等参数
3. **批量操作**：使用批量插入提高吞吐量

### Weaviate：

1. **分片配置**：合理设置数据分片
2. **缓存策略**：利用查询缓存
3. **模块选择**：根据场景选择向量化模块

两者都有活跃的社区和良好的文档支持，可以根据具体需求选择。对于纯向量搜索场景，Milvus 性能更优；对于需要多模态和AI集成的应用，Weaviate 更加方便。



---



### Weaviate配置

```yaml
version: '3.4'
services:
  weaviate:
    command:
    - --host
    - 0.0.0.0
    - --port
    - '8080'
    - --scheme
    - http
    image: cr.weaviate.io/semitechnologies/weaviate:1.24.1
    ports:
    - 8080:8080
    - 50051:50051
    restart: on-failure:0
    environment:
      QUERY_DEFAULTS_LIMIT: 20
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'
      PERSISTENCE_DATA_PATH: '/var/lib/weaviate'
      DEFAULT_VECTORIZER_MODULE: 'text2vec-transformers'
      ENABLE_MODULES: 'text2vec-transformers,text2vec-openai,text2vec-cohere,generative-openai,qna-openai,ref2vec-centroid,reranker-transformers'
      TRANSFORMERS_INFERENCE_API: 'http://t2v-transformers:8080'
      CLUSTER_HOSTNAME: 'node1'
      OPENAI_APIKEY: ${OPENAI_API_KEY:-}
      COHERE_APIKEY: ${COHERE_APIKEY:-}
    volumes:
      - weaviate_data:/var/lib/weaviate
    depends_on:
      - t2v-transformers

  t2v-transformers:
    image: cr.weaviate.io/semitechnologies/transformers-inference:sentence-transformers-multi-qa-MiniLM-L6-cos-v1
    environment:
      ENABLE_CUDA: 0
    ports:
      - "9090:8080"

  # 可选：添加一个简单的文本处理服务（用于预处理）
  text-processor:
    image: python:3.11-slim
    container_name: text-processor
    volumes:
      - ./text_processor:/app
    working_dir: /app
    command: >
      bash -c "pip install nltk sentence-transformers tiktoken &&
               python -c \"import nltk; nltk.download('punkt')\" &&
               tail -f /dev/null"
    ports:
      - "5000:5000"


  # 可选：添加Redis用于缓存（在LLM应用中很有用）
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes

volumes:
  weaviate_data:
  redis_data:
```

