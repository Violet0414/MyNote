#### **LlamaIndex是什么**

​		**LlamaIndex**是一个专门用于连接私有数据和LLM的框架。可以将其理解为LLM的数据接口或者数据层。

​		**解决了LLM的知识受限于本身训练数据，无法访问私有、实时的或训练数据中未包含的特定信息，其本身的核心思想就是RAG**。

---



#### 核心问题：为什么需要 LlamaIndex？

想象一下，您想用 ChatGPT 来分析您公司内部的财务报告、个人笔记或数据库。您会遇到以下问题：

1. **上下文长度限制：** LLM 有输入长度的限制，无法一次性将几百页的文档全部喂给它。
2. **数据隐私与成本：** 您不能将敏感的私有数据直接发送给公开的 API。
3. **无关信息干扰：** 即使能发送全部数据，大量无关信息也会干扰 LLM，导致回答不准确。
4. **结构化访问：** 如何让 LLM 理解复杂的数据结构（如 PDF、PPT、SQL 数据库、API 返回结果）并从中精准提取信息？

**LlamaIndex 就是为了解决这些问题而生的。**

---



#### 核心思想与工作原理：检索增强生成

LlamaIndex 的核心思想是 **RAG**。其工作流程可以概括为以下几个关键步骤：

1. **数据加载与连接：**
   - LlamaIndex 提供了大量的 **数据连接器**，可以从各种来源加载数据，包括 PDF、PPT、Word、Excel、数据库（SQL、NoSQL）、Notion、Slack、Google Docs 等。
   - 核心组件：`Reader` 和 `Loader`。
2. **索引与结构化：**（这是 LlamaIndex 最核心的价值）
   - 加载的原始数据（文档）会被解析、分割成更小的、有意义的“块”。
   - 然后，它为这些数据块创建一种中间表示形式，即 **索引**。最常用的是**向量索引**。
   - **向量化：** 使用嵌入模型将每个文本块转换为一个高维向量（一组数字）。语义相似的文本块，其向量在空间中的距离也更近。
   - 这个过程将您的非结构化数据（文本）转换成了 LLM 能够高效理解和检索的“结构化”格式。
3. **存储：**
   - 创建好的索引（尤其是向量）会被存储起来，通常是本地的向量数据库或内存中，方便后续快速查询。
4. **查询与检索：**
   - 当您提出一个问题时，LlamaIndex 会：
     - **检索：** 将您的问题也转换为向量，然后在索引中寻找与之最相似的文本块（即向量最接近的块）。
     - **合成：** 将这些检索到的、最相关的文本块作为“上下文”，与您的原始问题一起打包，发送给 LLM。
     - **生成：** LLM 基于您提供的精准上下文来生成答案，而不是依赖其内部知识。

---



### LlamaIndex常用组件

#### 	PromptTemplate(提示词组件)

​			我们可以自己配置一些提示词模板，使得提示词能够有一个自定义的标准和模板。

```python
template = ("把语句\"{text}\"翻译成{language}")
# 构建提示词模板
prompt_template = PromptTemplate(template=template)
# 将提示词模板格式化具体提示词
prompt = prompt_template.format(text='Hello World', language='中文')
print(prompt);
```











