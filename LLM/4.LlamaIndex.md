#### **LlamaIndexæ˜¯ä»€ä¹ˆ**

â€‹		**LlamaIndex**æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¿æ¥ç§æœ‰æ•°æ®å’ŒLLMçš„æ¡†æ¶ã€‚å¯ä»¥å°†å…¶ç†è§£ä¸ºLLMçš„æ•°æ®æ¥å£æˆ–è€…æ•°æ®å±‚ã€‚

â€‹		**è§£å†³äº†LLMçš„çŸ¥è¯†å—é™äºæœ¬èº«è®­ç»ƒæ•°æ®ï¼Œæ— æ³•è®¿é—®ç§æœ‰ã€å®æ—¶çš„æˆ–è®­ç»ƒæ•°æ®ä¸­æœªåŒ…å«çš„ç‰¹å®šä¿¡æ¯ï¼Œå…¶æœ¬èº«çš„æ ¸å¿ƒæ€æƒ³å°±æ˜¯RAG**ã€‚

---



#### æ ¸å¿ƒé—®é¢˜ï¼šä¸ºä»€ä¹ˆéœ€è¦ LlamaIndexï¼Ÿ

æƒ³è±¡ä¸€ä¸‹ï¼Œæ‚¨æƒ³ç”¨ ChatGPT æ¥åˆ†ææ‚¨å…¬å¸å†…éƒ¨çš„è´¢åŠ¡æŠ¥å‘Šã€ä¸ªäººç¬”è®°æˆ–æ•°æ®åº“ã€‚æ‚¨ä¼šé‡åˆ°ä»¥ä¸‹é—®é¢˜ï¼š

1. **ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶ï¼š** LLM æœ‰è¾“å…¥é•¿åº¦çš„é™åˆ¶ï¼Œæ— æ³•ä¸€æ¬¡æ€§å°†å‡ ç™¾é¡µçš„æ–‡æ¡£å…¨éƒ¨å–‚ç»™å®ƒã€‚
2. **æ•°æ®éšç§ä¸æˆæœ¬ï¼š** æ‚¨ä¸èƒ½å°†æ•æ„Ÿçš„ç§æœ‰æ•°æ®ç›´æ¥å‘é€ç»™å…¬å¼€çš„ APIã€‚
3. **æ— å…³ä¿¡æ¯å¹²æ‰°ï¼š** å³ä½¿èƒ½å‘é€å…¨éƒ¨æ•°æ®ï¼Œå¤§é‡æ— å…³ä¿¡æ¯ä¹Ÿä¼šå¹²æ‰° LLMï¼Œå¯¼è‡´å›ç­”ä¸å‡†ç¡®ã€‚
4. **ç»“æ„åŒ–è®¿é—®ï¼š** å¦‚ä½•è®© LLM ç†è§£å¤æ‚çš„æ•°æ®ç»“æ„ï¼ˆå¦‚ PDFã€PPTã€SQL æ•°æ®åº“ã€API è¿”å›ç»“æœï¼‰å¹¶ä»ä¸­ç²¾å‡†æå–ä¿¡æ¯ï¼Ÿ

**LlamaIndex å°±æ˜¯ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜è€Œç”Ÿçš„ã€‚**

---



#### æ ¸å¿ƒæ€æƒ³ä¸å·¥ä½œåŸç†ï¼šæ£€ç´¢å¢å¼ºç”Ÿæˆ

LlamaIndex çš„æ ¸å¿ƒæ€æƒ³æ˜¯ **RAG**ã€‚å…¶å·¥ä½œæµç¨‹å¯ä»¥æ¦‚æ‹¬ä¸ºä»¥ä¸‹å‡ ä¸ªå…³é”®æ­¥éª¤ï¼š

1. **æ•°æ®åŠ è½½ä¸è¿æ¥ï¼š**
   - LlamaIndex æä¾›äº†å¤§é‡çš„ **æ•°æ®è¿æ¥å™¨**ï¼Œå¯ä»¥ä»å„ç§æ¥æºåŠ è½½æ•°æ®ï¼ŒåŒ…æ‹¬ PDFã€PPTã€Wordã€Excelã€æ•°æ®åº“ï¼ˆSQLã€NoSQLï¼‰ã€Notionã€Slackã€Google Docs ç­‰ã€‚
   - æ ¸å¿ƒç»„ä»¶ï¼š`Reader` å’Œ `Loader`ã€‚
2. **ç´¢å¼•ä¸ç»“æ„åŒ–ï¼š**ï¼ˆè¿™æ˜¯ LlamaIndex æœ€æ ¸å¿ƒçš„ä»·å€¼ï¼‰
   - åŠ è½½çš„åŸå§‹æ•°æ®ï¼ˆæ–‡æ¡£ï¼‰ä¼šè¢«è§£æã€åˆ†å‰²æˆæ›´å°çš„ã€æœ‰æ„ä¹‰çš„â€œå—â€ã€‚
   - ç„¶åï¼Œå®ƒä¸ºè¿™äº›æ•°æ®å—åˆ›å»ºä¸€ç§ä¸­é—´è¡¨ç¤ºå½¢å¼ï¼Œå³ **ç´¢å¼•**ã€‚æœ€å¸¸ç”¨çš„æ˜¯**å‘é‡ç´¢å¼•**ã€‚
   - **å‘é‡åŒ–ï¼š** ä½¿ç”¨åµŒå…¥æ¨¡å‹å°†æ¯ä¸ªæ–‡æœ¬å—è½¬æ¢ä¸ºä¸€ä¸ªé«˜ç»´å‘é‡ï¼ˆä¸€ç»„æ•°å­—ï¼‰ã€‚è¯­ä¹‰ç›¸ä¼¼çš„æ–‡æœ¬å—ï¼Œå…¶å‘é‡åœ¨ç©ºé—´ä¸­çš„è·ç¦»ä¹Ÿæ›´è¿‘ã€‚
   - è¿™ä¸ªè¿‡ç¨‹å°†æ‚¨çš„éç»“æ„åŒ–æ•°æ®ï¼ˆæ–‡æœ¬ï¼‰è½¬æ¢æˆäº† LLM èƒ½å¤Ÿé«˜æ•ˆç†è§£å’Œæ£€ç´¢çš„â€œç»“æ„åŒ–â€æ ¼å¼ã€‚
3. **å­˜å‚¨ï¼š**
   - åˆ›å»ºå¥½çš„ç´¢å¼•ï¼ˆå°¤å…¶æ˜¯å‘é‡ï¼‰ä¼šè¢«å­˜å‚¨èµ·æ¥ï¼Œé€šå¸¸æ˜¯æœ¬åœ°çš„å‘é‡æ•°æ®åº“æˆ–å†…å­˜ä¸­ï¼Œæ–¹ä¾¿åç»­å¿«é€ŸæŸ¥è¯¢ã€‚
4. **æŸ¥è¯¢ä¸æ£€ç´¢ï¼š**
   - å½“æ‚¨æå‡ºä¸€ä¸ªé—®é¢˜æ—¶ï¼ŒLlamaIndex ä¼šï¼š
     - **æ£€ç´¢ï¼š** å°†æ‚¨çš„é—®é¢˜ä¹Ÿè½¬æ¢ä¸ºå‘é‡ï¼Œç„¶ååœ¨ç´¢å¼•ä¸­å¯»æ‰¾ä¸ä¹‹æœ€ç›¸ä¼¼çš„æ–‡æœ¬å—ï¼ˆå³å‘é‡æœ€æ¥è¿‘çš„å—ï¼‰ã€‚
     - **åˆæˆï¼š** å°†è¿™äº›æ£€ç´¢åˆ°çš„ã€æœ€ç›¸å…³çš„æ–‡æœ¬å—ä½œä¸ºâ€œä¸Šä¸‹æ–‡â€ï¼Œä¸æ‚¨çš„åŸå§‹é—®é¢˜ä¸€èµ·æ‰“åŒ…ï¼Œå‘é€ç»™ LLMã€‚
     - **ç”Ÿæˆï¼š** LLM åŸºäºæ‚¨æä¾›çš„ç²¾å‡†ä¸Šä¸‹æ–‡æ¥ç”Ÿæˆç­”æ¡ˆï¼Œè€Œä¸æ˜¯ä¾èµ–å…¶å†…éƒ¨çŸ¥è¯†ã€‚

---



### LlamaIndexå¸¸ç”¨ç»„ä»¶

#### 	PromptTemplate(æç¤ºè¯ç»„ä»¶)

â€‹			æˆ‘ä»¬å¯ä»¥è‡ªå·±é…ç½®ä¸€äº›æç¤ºè¯æ¨¡æ¿ï¼Œä½¿å¾—æç¤ºè¯èƒ½å¤Ÿæœ‰ä¸€ä¸ªè‡ªå®šä¹‰çš„æ ‡å‡†å’Œæ¨¡æ¿ã€‚

```python
template = ("æŠŠè¯­å¥\"{text}\"ç¿»è¯‘æˆ{language}")
# æ„å»ºæç¤ºè¯æ¨¡æ¿
prompt_template = PromptTemplate(template=template)
# å°†æç¤ºè¯æ¨¡æ¿æ ¼å¼åŒ–å…·ä½“æç¤ºè¯
prompt = prompt_template.format(text='Hello World', language='ä¸­æ–‡')
print(prompt);
```

**æç¤ºè¯(message)ç±»å‹ï¼š**

â€‹	user messageï¼šç”¨æˆ·è¾“å…¥çš„æ¶ˆæ¯ã€‚

â€‹	system messageï¼š æ¨¡å‹çš„ç›®æ ‡æˆ–è§’è‰²ã€‚

â€‹	assistant messageï¼šæ¨¡å‹å¯¹ç”¨æˆ·æ¶ˆæ¯çš„å›å¤ã€‚

â€‹	tool messageï¼šå·¥å…·è¾“å‡ºçš„ä¿¡æ¯ã€‚



#### ChatPromptTemplate(å¤šæ¡æ¶ˆæ¯æ ¼å¼åŒ–ç»„ä»¶)

```python
from llama index.core import ChatPromptTemplate
from llama_index.core.llms import ChatMessage,MessageRole
# æ¶ˆæ¯è®°å½•
message_templates=[
    ChatMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¯­è¨€ç¿»è¯‘ï¼Œè´Ÿè´£å°†ç”¨æˆ·çš„è¯­å¥ç¿»è¯‘æˆ {language}."ï¼Œrole=MessageRole.SYSTEM),
    ChatMessage(content="{text}", role=MessageRole.USER,),
]
# æ„å»ºæ¨¡æ¿
chat_template = ChatPromptTemplate(message_templates=message_templates)
#æ ¼å¼åŒ–ä¸ºæ–‡æœ¬
prompt = chat_template.format(text="it is such a rainy day", language="ä¸­æ–‡")
print(prompt)
# æ ¼å¼åŒ–ä¸ºæ¶ˆæ¯
messages = chat _template.format_messages(text="it is such a rainy day", language="ä¸­æ–‡")
print(messages)
```



#### RichPromptTemplate(åŠ¨æ€å®šåˆ¶æç¤ºè¯ç»„ä»¶)

```python
from llama_index.core.prompts import RichPromptTemplate
prompt_tmpl = RichPromptTemplate("æŠŠç”¨æˆ·çš„é—®é¢˜:\"{{ text }}\"ç¿»è¯‘æˆ {{ language }}")
prompt_str = prompt_tmpl.format(text="it is such a rainy day", language="ä¸­æ–‡")
prompt_str
```



#### æç¤ºè¯å·¥ç¨‹

â€‹		ç³»ç»Ÿæ¥æ”¶åˆ°ç”¨æˆ·çš„é£æ ¼é€‰æ‹©ï¼ˆå¦‚â€œæ´»æ³¼â€ã€â€œä¸“ä¸šâ€ã€â€œç®€æ´â€ã€â€œå¹½é»˜â€ï¼‰åï¼Œå¹¶ä¸æ˜¯ç›´æ¥ç»™æ¨¡å‹ä¼ é€’ä¸€ä¸ªâ€œstyle=playfulâ€çš„å‚æ•°ã€‚è€Œæ˜¯**å°†è¿™ä¸ªæè¿°è½¬æ¢æˆä¸€æ®µç»“æ„åŒ–çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼Œå¹¶æ‹¼æ¥åˆ°ç”¨æˆ·åŸæœ¬çš„è¯·æ±‚ä¹‹å‰æˆ–ä¹‹å**ï¼Œå½¢æˆä¸€ä¸ªå®Œæ•´çš„æç¤ºè¯ï¼ˆPromptï¼‰ï¼Œå†å‘é€ç»™å¤§æ¨¡å‹ã€‚

```python
from llama_index.core import PromptTemplate

# 1. å®šä¹‰ä¸€ä¸ªåŒ…å«é£æ ¼å ä½ç¬¦çš„æç¤ºè¯æ¨¡æ¿
style_template = PromptTemplate("""ä½ æ˜¯ä¸€ä½{style}çš„åŠ©æ‰‹ã€‚è¯·ç”¨{style}çš„é£æ ¼å›åº”ç”¨æˆ·çš„é—®é¢˜ã€‚

ç”¨æˆ·é—®é¢˜ï¼š{query}
åŠ©æ‰‹å›å¤ï¼š""")

# 2. æ ¹æ®ç”¨æˆ·é€‰æ‹©ï¼Œå¡«å……æ¨¡æ¿
user_style = "æ´»æ³¼å¹½é»˜"  # è¿™ä¸ªå€¼æ¥è‡ªå‰ç«¯çš„ä¸‹æ‹‰æ¡†æˆ–æŒ‰é’®
user_query = "ä»‹ç»ä¸€ä¸‹å¤ªé˜³èƒ½å‘ç”µã€‚"

# 3. ç”Ÿæˆæœ€ç»ˆçš„æç¤ºè¯
final_prompt = style_template.format(style=user_style, query=user_query)

# 4. å°† final_prompt å‘é€ç»™LLMè¿›è¡Œç”Ÿæˆ
```

- **ä¸»æµæ–¹æ¡ˆï¼ˆ95%ä»¥ä¸Šï¼‰ï¼š** **æç¤ºè¯æ‹¼æ¥**ã€‚å› ä¸ºå®ƒé›¶æˆæœ¬ã€çµæ´»ã€æ— éœ€é‡æ–°è®­ç»ƒæ¨¡å‹ï¼Œå¹¶ä¸”å¯¹äºæŒ‡ä»¤éµå¾ªèƒ½åŠ›å¼ºçš„ç°ä»£LLMï¼ˆå¦‚GPT-4ã€Claudeã€DeepSeekï¼‰å·²ç»éå¸¸æœ‰æ•ˆã€‚
- **ç³»ç»Ÿè§’è‰²ï¼š** äº¤äº’ç³»ç»Ÿï¼ˆå‰ç«¯ï¼‰çš„å·¥ä½œæ˜¯**å°†ç”¨æˆ·å‹å¥½çš„é€‰æ‹©æ˜ å°„ä¸ºä¸€æ®µç²¾ç¡®çš„ã€æ¨¡å‹èƒ½ç†è§£çš„è‡ªç„¶è¯­è¨€æè¿°**ï¼Œå¹¶å°†å…¶åµŒå…¥åˆ°è¯·æ±‚çš„æç¤ºè¯ä¸­ã€‚
- **æ˜ å°„æ˜¯å…³é”®ï¼š** â€œæ´»æ³¼â€è¿™ä¸ªè¯æœ¬èº«å¯èƒ½å¾ˆæ¨¡ç³Šã€‚ä¸€ä¸ªä¼˜ç§€çš„ç³»ç»Ÿåå°ï¼Œå¯èƒ½ä¼šå°†â€œæ´»æ³¼â€æ˜ å°„ä¸ºä¸€æ®µæ›´è¯¦ç»†çš„æŒ‡ä»¤ï¼Œä¾‹å¦‚ï¼šâ€œä½¿ç”¨è½»æ¾æ„‰å¿«çš„è¯­æ°”ï¼Œå¤šä½¿ç”¨æ„Ÿå¹è¯å’Œemojiï¼Œè¯­è¨€å£è¯­åŒ–ï¼Œå¯ä»¥åŠ å…¥ä¸€äº›æœ‰è¶£çš„æ¯”å–»ã€‚â€ è¿™ç§æ˜ å°„å…³ç³»å¯èƒ½å­˜å‚¨åœ¨ä¸€ä¸ªé…ç½®å­—å…¸é‡Œã€‚

```python
# ä¸€ä¸ªæ›´ç²¾ç»†çš„é£æ ¼æ˜ å°„ç¤ºä¾‹
STYLE_PROMPT_MAP = {
    "æ´»æ³¼": "ä½¿ç”¨è½»æ¾æ„‰å¿«çš„è¯­æ°”ï¼Œå¤šä½¿ç”¨æ„Ÿå¹è¯å’Œemojiï¼Œè¯­è¨€å£è¯­åŒ–ï¼Œå¯ä»¥åŠ å…¥ä¸€äº›æœ‰è¶£çš„æ¯”å–»ã€‚",
    "ä¸“ä¸š": "ä½¿ç”¨æ­£å¼ã€ä¸¥è°¨ã€å®¢è§‚çš„å­¦æœ¯æˆ–å•†ä¸šç”¨è¯­ï¼Œé¿å…å£è¯­åŒ–è¡¨è¾¾ï¼Œç»“æ„æ¸…æ™°ï¼Œåˆ†ç‚¹è®ºè¿°ã€‚",
    "ç®€æ´": "ç›´æ¥ç»™å‡ºæ ¸å¿ƒç­”æ¡ˆï¼Œæ— éœ€èƒŒæ™¯é“ºå«æˆ–æ‰©å±•è§£é‡Šï¼Œé¿å…ä¿®è¾ï¼ŒåŠ›æ±‚è¨€ç®€æ„èµ…ã€‚",
    "èå£«æ¯”äºš": "æ¨¡ä»¿èå£«æ¯”äºšæˆå‰§çš„æ–‡å­¦é£æ ¼ï¼Œä½¿ç”¨å¤å…¸çš„è‹±è¯­è¡¨è¾¾ã€æ¯”å–»å’Œä¿®è¾æ‰‹æ³•ã€‚",
}

def build_prompt(query, style_key):
    style_instruction = STYLE_PROMPT_MAP.get(style_key, "")
    return f"{style_instruction}\n\nç”¨æˆ·é—®é¢˜ï¼š{query}"
```

---



#### Embeddings(å‘é‡åŒ–ç»„ä»¶)

**æ ¸å¿ƒåŸºç±»**

```python
from llama_index.core.embeddings import BaseEmbedding

class BaseEmbedding:
    """æ‰€æœ‰embeddingæ¨¡å‹çš„åŸºç±»"""
    
    def get_text_embedding(self, text: str) -> List[float]:
        """å•ä¸ªæ–‡æœ¬å‘é‡åŒ–"""
        
    def get_text_embedding_batch(self, texts: List[str]) -> List[List[float]]:
        """æ‰¹é‡æ–‡æœ¬å‘é‡åŒ–"""
        
    def get_agg_embedding_from_queries(self, *args) -> List[float]:
        """ä»æŸ¥è¯¢èšåˆembedding"""
```



**äº‘ç«¯APIæ¨¡å‹**

```python
# OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding
embed_model = OpenAIEmbedding(
    model="text-embedding-3-small",
    api_key="sk-...",
    embed_batch_size=100  # æ‰¹é‡å¤§å°
)

# Azure OpenAI
from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding
embed_model = AzureOpenAIEmbedding(
    model="text-embedding-3-small",
    deployment_name="your-deployment",
    api_key="...",
    azure_endpoint="https://xxx.openai.azure.com/"
)

# Cohere
from llama_index.embeddings.cohere import CohereEmbedding
embed_model = CohereEmbedding(
    cohere_api_key="...",
    model_name="embed-english-v3.0",
    input_type="search_document"
)

# ç™¾åº¦æ–‡å¿ƒ
from llama_index.embeddings.baidu_qianfan import QianfanEmbedding
embed_model = QianfanEmbedding(
    model="Embedding-V1",
    qianfan_ak="...",
    qianfan_sk="..."
)
```



**æœ¬åœ°æ¨¡å‹**

```python
# HuggingFace
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
embed_model = HuggingFaceEmbedding(
    model_name="BAAI/bge-large-zh-v1.5",
    device="cuda",  # å¯é€‰ï¼šcuda, cpu, mps
    trust_remote_code=True,
    cache_folder="./hf_cache"
)

# Sentence Transformers
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
embed_model = HuggingFaceEmbedding(
    model_name="sentence-transformers/all-mpnet-base-v2",
    normalize_embeddings=True
)

# è‡ªå®šä¹‰æœ¬åœ°æ¨¡å‹
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
embed_model = HuggingFaceEmbedding(
    model_name="/path/to/your/model",
    tokenizer_name="/path/to/your/tokenizer"
)
```

---



**å‘é‡åŒ–**

```python
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
import torch

# ä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„
embed_model = HuggingFaceEmbedding(
    model_name="./model",  # æœ¬åœ°æ¨¡å‹æ–‡ä»¶å¤¹è·¯å¾„
    device="cuda" if torch.cuda.is_available() else "cpu",  # è‡ªåŠ¨é€‰æ‹©è®¾å¤‡
    trust_remote_code=True,  # å¦‚æœæ¨¡å‹éœ€è¦è‡ªå®šä¹‰ä»£ç 
    model_kwargs={
        "torch_dtype": torch.float16,  # åŠç²¾åº¦èŠ‚çœå†…å­˜
        "local_files_only": True,      # å¼ºåˆ¶ä½¿ç”¨æœ¬åœ°æ–‡ä»¶
    },
    encode_kwargs={
        "normalize_embeddings": True,  # å½’ä¸€åŒ–å‘é‡
        "batch_size": 32,
        "show_progress_bar": True
    }
)

# æµ‹è¯•å‘é‡åŒ–
texts = ["è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å¥å­", "è¿™æ˜¯å¦ä¸€ä¸ªå¥å­"]
embeddings = embed_model.get_text_embedding_batch(texts)

print(f"Embeddingç»´åº¦: {len(embeddings[0])}")
print(f"å‰5ä¸ªå€¼: {embeddings[0][:5]}")
```



## åŸºæœ¬äº¤äº’ç¤ºä¾‹

```python
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.llms.openai import OpenAI
from llama_index.core import Settings
import os

# 1. é…ç½® LLMï¼ˆè¿™é‡Œä»¥ OpenAI ä¸ºä¾‹ï¼‰
# è®¾ç½®ä½ çš„ OpenAI API key
os.environ["OPENAI_API_KEY"] = "your-api-key"

# åˆ›å»º LLM å®ä¾‹
llm = OpenAI(model="gpt-3.5-turbo", temperature=0.7)

# è®¾ç½®ä¸ºå…¨å±€ LLM
Settings.llm = llm

# 2. åŠ è½½æ–‡æ¡£ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸éœ€è¦ RAG å¯ä»¥è·³è¿‡ï¼‰
# ä»æ–‡ä»¶å¤¹åŠ è½½æ–‡æ¡£
documents = SimpleDirectoryReader("./data").load_data()

# 3. åˆ›å»ºç´¢å¼•
index = VectorStoreIndex.from_documents(documents)

# 4. åˆ›å»ºæŸ¥è¯¢å¼•æ“
query_engine = index.as_query_engine()

# 5. äº¤äº’å¼é—®ç­”
while True:
    # è·å–ç”¨æˆ·è¾“å…¥
    query = input("\nè¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼ˆè¾“å…¥ 'quit' é€€å‡ºï¼‰: ")
    
    if query.lower() in ['quit', 'exit', 'q']:
        print("å†è§ï¼")
        break
    
    if not query.strip():
        continue
    
    # æ‰§è¡ŒæŸ¥è¯¢
    try:
        print("\nğŸ¤– æ­£åœ¨æ€è€ƒ...")
        response = query_engine.query(query)
        print(f"\nğŸ’¡ å›ç­”: {response}")
    except Exception as e:
        print(f"âŒ å‡ºé”™: {e}")
```



## å¦‚æœä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼ˆå¦‚ HuggingFaceï¼‰

```python
from llama_index.core import Settings
from llama_index.llms.huggingface import HuggingFaceLLM
from llama_index.core import PromptTemplate
import torch

# 1. é…ç½®æœ¬åœ° LLM
llm = HuggingFaceLLM(
    model_name="Qwen/Qwen2-7B-Instruct",  # æˆ–ä½¿ç”¨æœ¬åœ°è·¯å¾„ "./model"
    tokenizer_name="Qwen/Qwen2-7B-Instruct",
    context_window=4096,
    max_new_tokens=256,
    generate_kwargs={"temperature": 0.7, "do_sample": True},
    device_map="auto",
)

# è®¾ç½®ä¸ºå…¨å±€ LLM
Settings.llm = llm

# 2. ç®€å•çš„ç›´æ¥é—®ç­”ï¼ˆä¸ä½¿ç”¨ RAGï¼‰
def simple_chat():
    """ç®€å•çš„å¯¹è¯äº¤äº’"""
    print("ğŸ¤– æœ¬åœ°æ¨¡å‹èŠå¤©æœºå™¨äººï¼ˆè¾“å…¥ 'quit' é€€å‡ºï¼‰")
    print("=" * 50)
    
    while True:
        try:
            # è·å–ç”¨æˆ·è¾“å…¥
            user_input = input("\nğŸ‘¤ ä½ : ").strip()
            
            if user_input.lower() in ['quit', 'exit', 'q']:
                print("ğŸ¤– å†è§ï¼")
                break
            
            if not user_input:
                continue
            
            # æ„é€ æç¤ºè¯
            prompt = f"""è¯·å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š
            
é—®é¢˜ï¼š{user_input}

å›ç­”ï¼š"""
            
            # ç”Ÿæˆå›ç­”
            print("ğŸ¤– æ­£åœ¨æ€è€ƒ...")
            response = llm.complete(prompt)
            
            print(f"ğŸ¤– æœºå™¨äºº: {response.text}")
            
        except KeyboardInterrupt:
            print("\nğŸ¤– å†è§ï¼")
            break
        except Exception as e:
            print(f"âŒ å‡ºé”™: {e}")

# è¿è¡ŒèŠå¤©
if __name__ == "__main__":
    simple_chat()
```



## ä½¿ç”¨æœ¬åœ°å‘é‡çŸ¥è¯†çš„ RAG é—®ç­”

```python
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.llms.huggingface import HuggingFaceLLM
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core import Settings
import torch

# 1. é…ç½®æœ¬åœ°æ¨¡å‹
print("ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹...")

# é…ç½® LLM
llm = HuggingFaceLLM(
    model_name="Qwen/Qwen2-7B-Instruct",  # å¯æ›¿æ¢ä¸ºä½ çš„æœ¬åœ°æ¨¡å‹è·¯å¾„
    tokenizer_name="Qwen/Qwen2-7B-Instruct",
    context_window=4096,
    max_new_tokens=512,
    device_map="auto",
)

# é…ç½® Embedding æ¨¡å‹
embed_model = HuggingFaceEmbedding(
    model_name="BAAI/bge-small-zh-v1.5",  # å¯æ›¿æ¢ä¸ºä½ çš„æœ¬åœ° embedding æ¨¡å‹
    device="cuda" if torch.cuda.is_available() else "cpu",
)

# è®¾ç½®å…¨å±€é…ç½®
Settings.llm = llm
Settings.embed_model = embed_model
Settings.chunk_size = 512
Settings.chunk_overlap = 50

# 2. åŠ è½½æœ¬åœ°çŸ¥è¯†åº“
print("ğŸ“š æ­£åœ¨åŠ è½½çŸ¥è¯†åº“...")
try:
    # ä» data æ–‡ä»¶å¤¹åŠ è½½æ–‡æ¡£
    documents = SimpleDirectoryReader("./data").load_data()
    
    # åˆ›å»ºç´¢å¼•
    index = VectorStoreIndex.from_documents(
        documents,
        show_progress=True
    )
    
    # åˆ›å»ºæŸ¥è¯¢å¼•æ“
    query_engine = index.as_query_engine(
        similarity_top_k=3,  # ä½¿ç”¨å‰3ä¸ªæœ€ç›¸å…³çš„æ–‡æ¡£
        verbose=True  # æ˜¾ç¤ºè¯¦ç»†è¿‡ç¨‹
    )
    
    print("âœ… ç³»ç»Ÿå‡†å¤‡å°±ç»ªï¼")
    
except Exception as e:
    print(f"âŒ åŠ è½½çŸ¥è¯†åº“å¤±è´¥: {e}")
    print("âš ï¸  å°†ä½¿ç”¨æ— çŸ¥è¯†åº“æ¨¡å¼")
    query_engine = None

# 3. äº¤äº’å¼é—®ç­”
def rag_chat():
    """åŸºäºçŸ¥è¯†åº“çš„é—®ç­”"""
    print("\n" + "="*50)
    print("ğŸ¤– æ™ºèƒ½çŸ¥è¯†é—®ç­”ç³»ç»Ÿ")
    print("="*50)
    print("æç¤ºï¼š")
    print("1. è¾“å…¥é—®é¢˜è·å–åŸºäºçŸ¥è¯†åº“çš„å›ç­”")
    print("2. è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
    print("="*50)
    
    while True:
        try:
            # è·å–ç”¨æˆ·é—®é¢˜
            question = input("\nâ“ è¯·è¾“å…¥é—®é¢˜: ").strip()
            
            if question.lower() in ['quit', 'exit', 'q']:
                print("ğŸ¤– æ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§ï¼")
                break
            
            if not question:
                continue
            
            print("\nğŸ” æ­£åœ¨æ£€ç´¢ä¿¡æ¯...")
            
            if query_engine:
                # ä½¿ç”¨çŸ¥è¯†åº“å›ç­”é—®é¢˜
                response = query_engine.query(question)
                print(f"\nğŸ’¡ å›ç­”: {response}")
                
                # æ˜¾ç¤ºå‚è€ƒæ¥æºï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if hasattr(response, 'source_nodes') and response.source_nodes:
                    print("\nğŸ“š å‚è€ƒæ¥æº:")
                    for i, node in enumerate(response.source_nodes[:2]):  # æ˜¾ç¤ºå‰2ä¸ªæ¥æº
                        print(f"{i+1}. {node.text[:150]}...")
            else:
                # æ— çŸ¥è¯†åº“ï¼Œç›´æ¥ä½¿ç”¨ LLM
                prompt = f"""è¯·å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š

é—®é¢˜ï¼š{question}

å›ç­”ï¼š"""
                response = llm.complete(prompt)
                print(f"\nğŸ’¡ å›ç­”: {response.text}")
                
        except KeyboardInterrupt:
            print("\nğŸ¤– å†è§ï¼")
            break
        except Exception as e:
            print(f"âŒ å‡ºé”™: {e}")

# è¿è¡Œ
if __name__ == "__main__":
    rag_chat()
```



## æœ€ç®€å•ç›´æ¥çš„ç‰ˆæœ¬ï¼ˆæ— çŸ¥è¯†åº“ï¼‰

```python
from llama_index.llms.huggingface import HuggingFaceLLM
import torch

# 1. åˆå§‹åŒ–æ¨¡å‹
print("ğŸš€ æ­£åœ¨åŠ è½½æ¨¡å‹...")
llm = HuggingFaceLLM(
    model_name="Qwen/Qwen2-7B-Instruct",  # æ›¿æ¢ä¸ºä½ çš„æ¨¡å‹è·¯å¾„
    tokenizer_name="Qwen/Qwen2-7B-Instruct",
    device_map="auto",
)

print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")

# 2. ç®€å•çš„é—®ç­”å¾ªç¯
while True:
    question = input("\nğŸ‘¤ ä½ é—®: ").strip()
    
    if question.lower() in ['é€€å‡º', 'quit', 'exit', 'q']:
        print("ğŸ¤– å†è§ï¼")
        break
    
    if not question:
        continue
    
    try:
        print("ğŸ¤– æ­£åœ¨æ€è€ƒ...")
        
        # ç›´æ¥ä½¿ç”¨ LLM ç”Ÿæˆå›ç­”
        response = llm.complete(question)
        
        print(f"ğŸ¤– å›ç­”: {response.text}")
        
    except Exception as e:
        print(f"âŒ å‡ºé”™: {e}")
```











â€‹	









