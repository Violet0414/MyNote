### RNN与CNN的区别

- **CNN(卷积神经网络) 像是一个具有“局部视野”和“平移不变性”的扫描仪**。它擅长在数据中（如图像）寻找局部特征（如边缘、角落），并且无论这个特征出现在哪个位置，它都能识别出来。
- **RNN(循环神经网络) 像是一个有“记忆”的处理器**。它按顺序处理数据（如句子中的单词），并将之前步骤的信息传递到当前步骤，以此来理解上下文和序列依赖关系。

---



### 核心区别对比表

| 特性               | CNN（卷积神经网络）                                          | RNN（循环神经网络）                                          |
| :----------------- | :----------------------------------------------------------- | :----------------------------------------------------------- |
| **核心设计理念**   | **局部连接** 和 **权重共享**                                 | **循环连接**，引入“时间步”或“顺序”概念                       |
| **主要处理的数据** | **网格状数据**                                               | **序列数据**                                                 |
| **典型应用领域**   | 图像分类、目标检测、图像分割                                 | 机器翻译、文本生成、语音识别、时间序列预测                   |
| **数据顺序敏感性** | **不敏感**（打乱图像像素，空间信息就丢失了，但模型本身不天然理解顺序） | **高度敏感**（单词或时间点的顺序至关重要）                   |
| **输入/输出长度**  | 通常是固定尺寸（如图像resize到224x224）                      | 可变长度（句子可长可短）                                     |
| **记忆机制**       | **无内部状态记忆**。每次推理是独立的。                       | **有内部隐藏状态**，作为“记忆”在时间步间传递。               |
| **并行化能力**     | **极高**。卷积核可以在整个输入上同时滑动计算。               | **差**。传统RNN必须按时间步**顺序**计算，无法并行。          |
| **处理长程依赖**   | 通过堆叠层数来扩大感受野，间接捕获长距离信息。               | **传统RNN很差**，会遭遇梯度消失/爆炸问题。LSTM/GRU有所改善，但仍有难度。 |
| **结构示意图核心** | **卷积核滑动**，提取局部特征，后接池化层降维。               | **循环单元**，当前输出依赖于当前输入和上一时刻的隐藏状态。   |

---



### 深入原理剖析

#### 1. CNN：空间特征的提取专家

- **局部连接**：不像全连接网络那样每个神经元都连接上一层的所有像素，CNN的每个神经元只连接输入数据的一个小区域（如3x3的窗口）。这大大减少了参数数量，并符合图像的局部性原理（一个像素只与它周围的像素关系密切）。
- **权重共享**：同一个卷积核（过滤器）会扫过整个输入图像。这意味着无论这个特征（如“垂直边缘”）出现在图像的左上角还是右下角，都是由同一个卷积核检测出来的。这赋予了CNN**平移不变性**。
- **层级结构**：浅层CNN学习到的是边缘、颜色等低级特征，中层学习到的是纹理、部件，深层学习到的是物体、整体等高级特征。这种层级结构非常契合图像的构成方式。

**简单总结：CNN的核心是“在空间上共享参数的局部特征提取器”。**

#### 2. RNN：序列依赖的建模专家

- **循环连接**：RNN的核心是一个循环单元，它有一个内部状态（隐藏状态 `h_t`）。在每一个时间步 `t`，它接收两个输入：**当前时间步的输入 `x_t`** 和 **上一个时间步的隐藏状态 `h_{t-1}`**。然后计算当前输出 `y_t` 和更新后的隐藏状态 `h_t`。
  - 公式化表示：`h_t = f(W * h_{t-1} + U * x_t + b)`
- **“记忆”的形成**：隐藏状态 `h_t` 可以被看作是网络对“到目前为止我所读到的序列”的总结或记忆。理论上，`h_t` 包含了之前所有输入 `x_1, x_2, ..., x_t` 的信息。
- **核心问题与改进**：
  - **梯度消失/爆炸**：当序列很长时，误差在反向传播时需要通过很多时间步，梯度会指数级地减小或增大，导致模型难以学习到长距离的依赖关系（例如，句首的词对句尾词的影响）。
  - **改进方案**：为了解决这个问题，更先进的RNN变体被提出，主要是 **LSTM（长短期记忆网络）** 和 **GRU（门控循环单元）**。它们通过引入“门”机制（输入门、遗忘门、输出门等）来有选择地记住或忘记信息，从而更有效地捕获长程依赖。

**简单总结：RNN的核心是“通过循环结构和内部状态来建模序列中的时间/顺序依赖关系”。**

---



### 示例

**句子：“我喜欢在公园里散步，因为它很____。”**

- **CNN的做法（如果我们强行把它用在文本上）**：

  - 它会用不同大小的“卷积核”扫描这句话的单词。例如，一个大小为3的卷积核可能会学习到“在公园里”这样一个局部短语的特征。
  - 但它很难确定“它”这个代词到底指代的是“公园”还是“散步”。因为要理解这一点，需要跨越很长的距离，将句首和句尾联系起来。

- **RNN（或LSTM）的做法**：

  - 它会按顺序处理每个词：“我” -> “喜欢” -> “在” -> ... -> “因为它很”。
  - 当处理到“它”这个词时，它的隐藏状态已经包含了前面所有词的信息（包括“公园”和“散步”）。通过分析上下文，它能更准确地判断出“它”指代的是“公园”，从而预测出下一个词应该是“大”、“安静”或“漂亮”等，而不是“累”（如果是“散步”的话）。

  ---

  

### 注意力机制

​		在RNN和CNN发展过程中都逐渐出现了存在注意力机制，其解决了以下问题：

1. 解决模型处理长序列时的”遗忘“问题：随着序列长度的增加，**远距离的依赖信息在传递过程中易被稀释，导致模型对长距离依赖关系的建模能力变弱。**
2. 解决不同时间步输入对当前时刻输出的“重要性”问题：**所有时间步的输入在计算当前时刻输出时被同等对待，忽略了不同时间步对当前时刻输出的重要性可能存在的差异。**



**人话版：解决了长时间没人叫我名字而忘记了我自己叫什么的问题，解决了我上完厕所可能拿手上的纸去擦嘴而不是擦屁股的问题。**



**固有缺陷：串行化计算，下一步的输入必须依赖于上一步的输出。**

















