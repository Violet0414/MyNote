### 前向/反向传播

**前向传播**：数据从神经网络输入层流向输出层的过程，目的是根据当前参数(权重和偏置)计算预测值。

类似按顺序执行计算逻辑，最终得到一个预测值。

**关键步骤：**

1. **输入数据**： 将样本数据（如图像像素、文本向量）输入网络。
2. **逐层计算**：
   - 每一层的神经元接收上一层的输出。
   - 进行 **线性计算**： `z = (权重 w * 输入 x) + 偏置 b`
   - 通过 **激活函数**（如 ReLU, Sigmoid）： `a = σ(z)`。这一步引入非线性，使网络可以拟合复杂关系。
3. **得到输出**： 数据经过所有隐藏层，最终到达输出层，产生一个预测值（如分类概率、回归值）。

**目的**：

- 得到网络的当前预测结果。
- 为反向传播准备好每一层的中间计算结果（`z`, `a`）。



**反向传播**：反向传播是误差从神经网络**输出层反向流回输入层**的过程，目的是计算损失函数相对于每个参数的**梯度**（即导数），从而知道如何调整参数才能减小预测误差。

类似得到答案后将输出的数据反向输入神经网络，从而得到计算误差以推导之前步骤某一参数的最佳值。

**关键步骤（链式法则的应用）：**

1. **计算损失**： 用损失函数（如均方误差、交叉熵）比较前向传播的**预测值**和真实的**标签值**，得到一个总的误差（损失）。
2. **反向传播误差**：
   - 从输出层开始，计算损失函数对输出层参数的梯度。
   - **利用链式法则**，将误差“分摊”回前一隐藏层：`(损失对输出的偏导) * (输出对输入的偏导) * (输入对权重的偏导)`。
   - 逐层重复此过程，直到传播回输入层。
3. **得到梯度**： 最终得到损失函数关于**所有权重和偏置**的梯度向量 `∇L`。这个梯度指示了“参数空间”中，哪个方向是让损失**增加最快**的方向。

**目的**：

- 计算每个参数的梯度 `∂L/∂w` 和 `∂L/∂b`。
- 这些梯度会立即用于**参数更新**（如下一步的梯度下降）。

































