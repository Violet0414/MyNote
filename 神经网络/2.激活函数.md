### 什么是激活函数

​		**激活函数**（Activation Function）是人工神经网络中的关键组件，它决定了神经元的输出信号。它的核心作用是**为神经网络引入非线性因素**，使网络能够学习和模拟复杂的现实世界数据关系。

------



### **为什么需要激活函数？**

​		如果没有激活函数，无论神经网络有多少层，它本质上只是一个**线性回归模型**（多个线性变换的叠加仍然是线性变换）。这样的模型无法处理现实世界中绝大多数非线性问题（如图像识别、自然语言处理等）。

**简单比喻：**

- 没有激活函数的神经网络 → 只能画**直线**
- 有激活函数的神经网络 → 可以画**任意复杂曲线**

------



### **激活函数的作用**

1. **引入非线性**：使网络可以逼近任意复杂函数。
2. **决定神经元是否被激活**：根据输入信号强度，决定是否传递信号给下一层。
3. **控制输出范围**：如将输出限制在 (0,1)、(-1,1) 等范围内。

------



### **常见的激活函数**

![](F:\MyNote\笔记图片\常见的激活函数.png)

------

### **如何选择激活函数？**

- **隐藏层**：目前 **ReLU** 及其变体（如 Leaky ReLU）是首选，因计算高效且缓解梯度消失。
- **输出层**：
  - 二分类：**Sigmoid**
  - 多分类：**Softmax**
  - 回归问题：线性激活（无激活函数）或 ReLU

------

### **重要概念补充**

- **梯度消失/爆炸**：在深层网络中，某些激活函数（如 Sigmoid、Tanh）的导数可能极小或极大，导致训练困难。ReLU 系列有效缓解了此问题。
- **稀疏激活**：ReLU 会使部分神经元输出为零，增加网络的稀疏性，可能提升效率和泛化能力



​		**激活函数最主要的作用就是为了使得神经网络可以将数据不再简单地线性分割，而是可以通过非线性函数将更好的区分数据特征，从而达到良好的训练效果。**